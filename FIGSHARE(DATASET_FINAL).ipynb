{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# In Cell 1, find this line:\n",
        "!pip install -q scipy scikit-learn opencv-python-headless tqdm h5py\n",
        "\n",
        "# Change it to this (just add 'mat73'):\n",
        "!pip install -q scipy scikit-learn opencv-python-headless tqdm h5py mat73"
      ],
      "metadata": {
        "id": "LW37Nbbl8d2q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mat73  # <-- NEW IMPORT\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "SOURCE_DIR = 'Downloaded_MAT_Files'\n",
        "OUTPUT_DIR = 'Figshare_Images'\n",
        "LABEL_MAP = {\n",
        "    1: 'meningioma',\n",
        "    2: 'glioma',\n",
        "    3: 'pituitary'\n",
        "}\n",
        "# --- End Configuration ---\n",
        "\n",
        "print(f\"--- Starting .mat to .jpg Conversion (Using mat73) ---\")\n",
        "print(f\"Source: {SOURCE_DIR}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "for class_name in LABEL_MAP.values():\n",
        "    os.makedirs(os.path.join(OUTPUT_DIR, class_name), exist_ok=True)\n",
        "\n",
        "patient_image_count = {}\n",
        "\n",
        "for i in tqdm.tqdm(range(1, 3065)):\n",
        "    mat_path = os.path.join(SOURCE_DIR, f\"{i}.mat\")\n",
        "\n",
        "    if not os.path.exists(mat_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # --- THIS IS THE FIX ---\n",
        "        # We are using mat73.loadmat, NOT scipy.io.loadmat\n",
        "        data = mat73.loadmat(mat_path)\n",
        "        # --- END FIX ---\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {mat_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    cjdata = data.get('cjdata')\n",
        "    if cjdata is None:\n",
        "        print(f\"Error: 'cjdata' not found in {mat_path}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Extract data (mat73 loads structs as dicts, so no [0][0])\n",
        "    try:\n",
        "        label = int(cjdata['label'])\n",
        "        pid = str(cjdata['PID'])\n",
        "        image = cjdata['image'].astype(np.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading struct from {mat_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # --- Image Processing ---\n",
        "    image_norm = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    image_8bit = np.uint8(image_norm)\n",
        "    image_rgb = cv2.cvtColor(image_8bit, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # --- File Naming ---\n",
        "    class_name = LABEL_MAP[label]\n",
        "\n",
        "    if pid not in patient_image_count:\n",
        "        patient_image_count[pid] = 0\n",
        "    patient_image_count[pid] += 1\n",
        "    img_num = patient_image_count[pid]\n",
        "\n",
        "    filename = f\"patient_{pid}_img_{img_num:04d}.jpg\"\n",
        "\n",
        "    # --- Save the File ---\n",
        "    save_path = os.path.join(OUTPUT_DIR, class_name, filename)\n",
        "    cv2.imwrite(save_path, image_rgb)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Conversion Complete!\")\n",
        "print(f\"All images saved in '{OUTPUT_DIR}' and sorted by class.\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q77QEKT8gIV",
        "outputId": "56a82d0c-ccf0-4ae0-fd08-b40a1e3dedd4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting .mat to .jpg Conversion (Using mat73) ---\n",
            "Source: Downloaded_MAT_Files\n",
            "Output: Figshare_Images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3064/3064 [00:37<00:00, 80.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Conversion Complete!\n",
            "All images saved in 'Figshare_Images' and sorted by class.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_patient_split(source_dir='Figshare_Images',\n",
        "                        output_dir='Figshare_Dataset',\n",
        "                        train_ratio=0.70,\n",
        "                        val_ratio=0.15,\n",
        "                        test_ratio=0.15,\n",
        "                        random_state=42):\n",
        "    \"\"\"\n",
        "    Creates a 3-way, patient-level, stratified split.\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(output_dir):\n",
        "        print(f\"Warning: Removing old directory: {output_dir}\")\n",
        "        shutil.rmtree(output_dir)\n",
        "\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 0.001, \\\n",
        "        \"Ratios must sum to 1.0!\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"CREATING PATIENT-LEVEL SPLIT (70/15/15)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    total_stats = {'Training': 0, 'Validation': 0, 'Testing': 0}\n",
        "\n",
        "    for class_name in os.listdir(source_dir):\n",
        "        class_path = os.path.join(source_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nProcessing class: {class_name}\")\n",
        "\n",
        "        patient_images = defaultdict(list)\n",
        "        for img_file in os.listdir(class_path):\n",
        "            if not (img_file.endswith('.jpg') or img_file.endswith('.png')):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                patient_id = img_file.split('_')[1]\n",
        "            except Exception as e:\n",
        "                print(f\"  Skipping malformed file: {img_file} ({e})\")\n",
        "                continue\n",
        "            patient_images[patient_id].append(img_file)\n",
        "\n",
        "        patients = list(patient_images.keys())\n",
        "        total_images = sum(len(imgs) for imgs in patient_images.values())\n",
        "\n",
        "        print(f\"  Total patients: {len(patients)}\")\n",
        "        print(f\"  Total images: {total_images}\")\n",
        "\n",
        "        train_patients, temp_patients = train_test_split(\n",
        "            patients,\n",
        "            test_size=(val_ratio + test_ratio),\n",
        "            random_state=random_state,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        val_patients, test_patients = train_test_split(\n",
        "            temp_patients,\n",
        "            test_size=test_ratio / (val_ratio + test_ratio),\n",
        "            random_state=random_state,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        print(f\"  Train patients: {len(train_patients)}\")\n",
        "        print(f\"  Val patients: {len(val_patients)}\")\n",
        "        print(f\"  Test patients: {len(test_patients)}\")\n",
        "\n",
        "        splits = {\n",
        "            'Training': train_patients,\n",
        "            'Validation': val_patients,\n",
        "            'Testing': test_patients\n",
        "        }\n",
        "\n",
        "        for split_name, patient_list in splits.items():\n",
        "            split_dir = os.path.join(output_dir, split_name, class_name)\n",
        "            os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "            image_count = 0\n",
        "            for patient_id in patient_list:\n",
        "                for img_file in patient_images[patient_id]:\n",
        "                    src = os.path.join(class_path, img_file)\n",
        "                    dst = os.path.join(split_dir, img_file)\n",
        "                    shutil.copy2(src, dst)\n",
        "                    image_count += 1\n",
        "\n",
        "            print(f\"  {split_name}: {image_count} images ({image_count/total_images*100:.1f}%)\")\n",
        "            total_stats[split_name] += image_count\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SPLIT COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    total_all = sum(total_stats.values())\n",
        "    for split_name, count in total_stats.items():\n",
        "        percentage = (count / total_all * 100) if total_all > 0 else 0\n",
        "        print(f\"{split_name}: {count} images ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"TOTAL: {total_all} images (Should be 3064)\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"✅ Your leak-proof dataset is ready at: {output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# --- RUN THE SPLITTING SCRIPT ---\n",
        "create_patient_split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIFbHP4z82vl",
        "outputId": "e7f12747-89b3-478f-e3f2-8fe5eb79c512"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CREATING PATIENT-LEVEL SPLIT (70/15/15)\n",
            "============================================================\n",
            "\n",
            "Processing class: pituitary\n",
            "  Total patients: 62\n",
            "  Total images: 930\n",
            "  Train patients: 43\n",
            "  Val patients: 9\n",
            "  Test patients: 10\n",
            "  Training: 648 images (69.7%)\n",
            "  Validation: 130 images (14.0%)\n",
            "  Testing: 152 images (16.3%)\n",
            "\n",
            "Processing class: meningioma\n",
            "  Total patients: 82\n",
            "  Total images: 708\n",
            "  Train patients: 57\n",
            "  Val patients: 12\n",
            "  Test patients: 13\n",
            "  Training: 509 images (71.9%)\n",
            "  Validation: 80 images (11.3%)\n",
            "  Testing: 119 images (16.8%)\n",
            "\n",
            "Processing class: glioma\n",
            "  Total patients: 89\n",
            "  Total images: 1426\n",
            "  Train patients: 62\n",
            "  Val patients: 13\n",
            "  Test patients: 14\n",
            "  Training: 1002 images (70.3%)\n",
            "  Validation: 223 images (15.6%)\n",
            "  Testing: 201 images (14.1%)\n",
            "\n",
            "============================================================\n",
            "SPLIT COMPLETE!\n",
            "============================================================\n",
            "Training: 2159 images (70.5%)\n",
            "Validation: 433 images (14.1%)\n",
            "Testing: 472 images (15.4%)\n",
            "TOTAL: 3064 images (Should be 3064)\n",
            "============================================================\n",
            "✅ Your leak-proof dataset is ready at: Figshare_Dataset\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_patient_ids_from_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Scans a directory (e.g., 'Training') and all its subfolders\n",
        "    (e.g., 'glioma') to find all unique patient IDs.\n",
        "    \"\"\"\n",
        "    patient_ids = set()\n",
        "\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder not found: {folder_path}\")\n",
        "        return patient_ids\n",
        "\n",
        "    for class_name in os.listdir(folder_path):\n",
        "        class_path = os.path.join(folder_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            for img_file in os.listdir(class_path):\n",
        "                try:\n",
        "                    patient_id = img_file.split('_')[1]\n",
        "                    patient_ids.add(patient_id)\n",
        "                except Exception:\n",
        "                    print(f\"  Could not parse ID from: {img_file}\")\n",
        "    return patient_ids\n",
        "\n",
        "# --- Main Verification Logic ---\n",
        "print(\"=\"*60)\n",
        "print(\"RUNNING PATIENT-LEVEL DATA LEAKAGE CHECK...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "DATASET_DIR = 'Figshare_Dataset'\n",
        "\n",
        "print(\"Scanning Training set...\")\n",
        "train_patients = get_patient_ids_from_folder(os.path.join(DATASET_DIR, 'Training'))\n",
        "print(f\"Found {len(train_patients)} unique patients in Training set.\\n\")\n",
        "\n",
        "print(\"Scanning Validation set...\")\n",
        "val_patients = get_patient_ids_from_folder(os.path.join(DATASET_DIR, 'Validation'))\n",
        "print(f\"Found {len(val_patients)} unique patients in Validation set.\\n\")\n",
        "\n",
        "print(\"Scanning Testing set...\")\n",
        "test_patients = get_patient_ids_from_folder(os.path.join(DATASET_DIR, 'Testing'))\n",
        "print(f\"Found {len(test_patients)} unique patients in Testing set.\\n\")\n",
        "\n",
        "# --- Leakage Analysis ---\n",
        "print(\"=\"*60)\n",
        "print(\"LEAKAGE ANALYSIS:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "tv_leakage = train_patients.intersection(val_patients)\n",
        "tt_leakage = train_patients.intersection(test_patients)\n",
        "vt_leakage = val_patients.intersection(test_patients)\n",
        "\n",
        "total_leakage = len(tv_leakage) + len(tt_leakage) + len(vt_leakage)\n",
        "\n",
        "if total_leakage == 0:\n",
        "    print(\"✅✅✅ SUCCESS! ✅✅✅\")\n",
        "    print(\"No patient-level data leakage found between any splits.\")\n",
        "else:\n",
        "    print(\"❌❌❌ FAILURE! DATA LEAKAGE DETECTED! ❌❌❌\")\n",
        "    if len(tv_leakage) > 0:\n",
        "        print(f\"  Leakage (Train <-> Val): {len(tv_leakage)} patients. {tv_leakage}\")\n",
        "    if len(tt_leakage) > 0:\n",
        "        print(f\"  Leakage (Train <-> Test): {len(tt_leakage)} patients. {tt_leakage}\")\n",
        "    if len(vt_leakage) > 0:\n",
        "        print(f\"  Leakage (Val <-> Test): {len(vt_leakage)} patients. {vt_leakage}\")\n",
        "\n",
        "print(\"\\n--- Verification Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MizD6cFo9LuU",
        "outputId": "8dbb170f-05de-4b25-f609-57c72c5d1f63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "RUNNING PATIENT-LEVEL DATA LEAKAGE CHECK...\n",
            "============================================================\n",
            "Scanning Training set...\n",
            "Found 162 unique patients in Training set.\n",
            "\n",
            "Scanning Validation set...\n",
            "Found 34 unique patients in Validation set.\n",
            "\n",
            "Scanning Testing set...\n",
            "Found 37 unique patients in Testing set.\n",
            "\n",
            "============================================================\n",
            "LEAKAGE ANALYSIS:\n",
            "============================================================\n",
            "✅✅✅ SUCCESS! ✅✅✅\n",
            "No patient-level data leakage found between any splits.\n",
            "\n",
            "--- Verification Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Install 'imagehash' library ---\n",
        "# This MUST be at the very top of the cell\n",
        "!pip install -q imagehash\n",
        "\n",
        "# --- Now we can import it ---\n",
        "import glob\n",
        "from PIL import Image\n",
        "import imagehash\n",
        "from collections import defaultdict\n",
        "import tqdm\n",
        "\n",
        "print(\"\\n--- Step 2: Scanning for Duplicate Images ---\")\n",
        "all_image_paths = glob.glob('Figshare_Dataset/**/*.jpg', recursive=True)\n",
        "hash_dict = defaultdict(list)\n",
        "\n",
        "print(f\"Calculating perceptual hashes for {len(all_image_paths)} images...\")\n",
        "for img_path in tqdm.tqdm(all_image_paths):\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "        hash_val = imagehash.phash(img)\n",
        "        hash_dict[hash_val].append(img_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not process {img_path}: {e}\")\n",
        "\n",
        "# --- Step 3: Reporting Duplicates ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DUPLICATE IMAGE ANALYSIS:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "duplicates_found = False\n",
        "total_duplicate_files = 0\n",
        "for hash_val, file_list in hash_dict.items():\n",
        "    if len(file_list) > 1:\n",
        "        duplicates_found = True\n",
        "        print(f\"\\n❌ Found Visually Identical Images (Hash: {hash_val}):\")\n",
        "        for file_path in file_list:\n",
        "            print(f\"  -> {file_path}\")\n",
        "        total_duplicate_files += len(file_list)\n",
        "\n",
        "if not duplicates_found:\n",
        "    print(\"✅✅✅ SUCCESS! ✅✅✅\")\n",
        "    print(\"No visually identical duplicate images were found in the dataset.\")\n",
        "else:\n",
        "    print(\"\\n--- Summary ---\")\n",
        "    print(f\"Found {total_duplicate_files} files that are visually identical.\")\n",
        "\n",
        "print(\"\\n--- Verification Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEhdahee9VOt",
        "outputId": "373f1e13-dceb-460e-a3ac-ce0955b0932a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/296.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m286.7/296.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "--- Step 2: Scanning for Duplicate Images ---\n",
            "Calculating perceptual hashes for 3064 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3064/3064 [00:11<00:00, 274.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DUPLICATE IMAGE ANALYSIS:\n",
            "============================================================\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: d0610b6e2f3c3f31):\n",
            "  -> Figshare_Dataset/Testing/pituitary/patient_103046_img_0003.jpg\n",
            "  -> Figshare_Dataset/Testing/pituitary/patient_103046_img_0002.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: 975a7aa161966768):\n",
            "  -> Figshare_Dataset/Testing/meningioma/patient_112552_img_0002.jpg\n",
            "  -> Figshare_Dataset/Testing/meningioma/patient_112552_img_0001.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: d23c2de065c7368e):\n",
            "  -> Figshare_Dataset/Testing/meningioma/patient_104281_img_0004.jpg\n",
            "  -> Figshare_Dataset/Testing/meningioma/patient_104281_img_0005.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: 85366a7a38e131cf):\n",
            "  -> Figshare_Dataset/Testing/glioma/patient_MR039473B_img_0002.jpg\n",
            "  -> Figshare_Dataset/Testing/glioma/patient_MR039473B_img_0003.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: c23a3de131c7668e):\n",
            "  -> Figshare_Dataset/Testing/glioma/patient_MR051586_img_0004.jpg\n",
            "  -> Figshare_Dataset/Testing/glioma/patient_MR051586_img_0003.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: d29e3dc9604d616d):\n",
            "  -> Figshare_Dataset/Testing/glioma/patient_MR029209E_img_0004.jpg\n",
            "  -> Figshare_Dataset/Testing/glioma/patient_MR029209E_img_0005.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: 91c23f924e39629f):\n",
            "  -> Figshare_Dataset/Training/pituitary/patient_103478_img_0009.jpg\n",
            "  -> Figshare_Dataset/Training/pituitary/patient_103478_img_0010.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: c49d5afe285a286b):\n",
            "  -> Figshare_Dataset/Training/pituitary/patient_112074_img_0013.jpg\n",
            "  -> Figshare_Dataset/Training/pituitary/patient_112074_img_0011.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: 950a7e792ce121e7):\n",
            "  -> Figshare_Dataset/Training/pituitary/patient_113274_img_0009.jpg\n",
            "  -> Figshare_Dataset/Training/pituitary/patient_113274_img_0008.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: c0e62f9b3d3c3099):\n",
            "  -> Figshare_Dataset/Training/meningioma/patient_107248_img_0004.jpg\n",
            "  -> Figshare_Dataset/Training/meningioma/patient_107248_img_0005.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: 901b6e646df16197):\n",
            "  -> Figshare_Dataset/Training/meningioma/patient_104695_img_0002.jpg\n",
            "  -> Figshare_Dataset/Training/meningioma/patient_104695_img_0003.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: 94c22b1e6b5b1e78):\n",
            "  -> Figshare_Dataset/Training/glioma/patient_MR049019G_img_0011.jpg\n",
            "  -> Figshare_Dataset/Training/glioma/patient_MR049019G_img_0012.jpg\n",
            "\n",
            "❌ Found Visually Identical Images (Hash: 86387be131c666d3):\n",
            "  -> Figshare_Dataset/Training/glioma/patient_MR051651B_img_0010.jpg\n",
            "  -> Figshare_Dataset/Training/glioma/patient_MR051651B_img_0009.jpg\n",
            "\n",
            "--- Summary ---\n",
            "Found 26 files that are visually identical.\n",
            "\n",
            "--- Verification Complete ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import time\n",
        "\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# This is the name of the file we will create in your Drive\n",
        "ZIP_FILE_PATH = '/content/drive/MyDrive/Figshare_Dataset_Clean.zip'\n",
        "print(f\"Your dataset will be saved to: {ZIP_FILE_PATH}\")\n",
        "\n",
        "print(\"\\nZipping the 'Figshare_Dataset' folder... (This may take 2-5 minutes)\")\n",
        "start_time = time.time()\n",
        "\n",
        "# -r = recursive (for folders)\n",
        "# -q = quiet (to hide the 3064 file names)\n",
        "!zip -rq {ZIP_FILE_PATH} Figshare_Dataset\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\n✅ SUCCESS!\")\n",
        "print(f\"File 'Figshare_Dataset_Clean.zip' is now saved in your Google Drive.\")\n",
        "print(f\"Zipping took {int(end_time - start_time)} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaF9VlFg9iey",
        "outputId": "27492f79-0f08-4bf7-84ee-22474c5ad8d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Your dataset will be saved to: /content/drive/MyDrive/Figshare_Dataset_Clean.zip\n",
            "\n",
            "Zipping the 'Figshare_Dataset' folder... (This may take 2-5 minutes)\n",
            "\n",
            "✅ SUCCESS!\n",
            "File 'Figshare_Dataset_Clean.zip' is now saved in your Google Drive.\n",
            "Zipping took 9 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import time\n",
        "\n",
        "ZIP_FILE_NAME = 'Figshare_Dataset_Clean.zip'\n",
        "\n",
        "print(\"Zipping the 'Figshare_Dataset' folder... (This may take 2-5 minutes)\")\n",
        "start_time = time.time()\n",
        "\n",
        "!zip -rq {ZIP_FILE_NAME} Figshare_Dataset\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Zipping took {int(end_time - start_time)} seconds.\")\n",
        "\n",
        "print(f\"\\nStarting download for {ZIP_FILE_NAME}...\")\n",
        "print(\"⚠️ This may take a very long time and can fail for large files.\")\n",
        "files.download(ZIP_FILE_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "l3pqRTXV-hQe",
        "outputId": "7986f72e-b06c-421d-926c-570f69c7fdce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipping the 'Figshare_Dataset' folder... (This may take 2-5 minutes)\n",
            "Zipping took 6 seconds.\n",
            "\n",
            "Starting download for Figshare_Dataset_Clean.zip...\n",
            "⚠️ This may take a very long time and can fail for large files.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8a1e4923-8bf5-4d19-9ea4-b80d3d1159c2\", \"Figshare_Dataset_Clean.zip\", 154011010)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}